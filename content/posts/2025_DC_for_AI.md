+++
title = 'Современный ЦОД для ИИ'
date = 2026-02-08
draft = false
tags = ["AI", "HW", "HPC"]
categories =  ["Искусcтвенный Интеллект", "Инфраструктура"]
+++

Вызов продиктован современными трендами развития ИИ инфраструктуры и потребностью строительства оптимизированных ЦОД.

Опорные данные:
1. В качестве сервера для расчетов взят сервер Nvidia DGX B200 и серверы с жидкостным охлаждением размером 4U SXM B200
2. Стартовое число размещаемых в ЦОД серверов: 100 штук
3. Среднегодовой рост числа серверов: 200 штук в год

Современный машинный зал для ИИ — это высокоплотная инженерная система, где критически важны энергоэффективность, максимальная плотность размещения оборудования и стратегический выбор архитектуры охлаждения. Для ЦОД ИИ со стартом на 100 серверов NVIDIA DGX B200 (10U в стойке) с ежегодным приростом 200 серверов и расчетом на 3 года, оптимальная инфраструктура требует жесткого следования ряду технических и экономических принципов. Так же рассмотрено размещение серверов с жидкостным охлаждением, более плотное размещение.

## Мощность на стойку и конфигурация машинного зала

Современные AI-серверы с видеокартами NVIDIA GB200 потребляют значительно больше энергии, чем традиционное оборудование. Стойка GB200 NVL72 с жидкостным охлаждением содержит 72 GPU Blackwell и потребляет около **120 кВт**, что в 10 раз превышает мощность классических серверных стоек (10-12 кВт). Для сравнения: стойки с воздушным охлаждением на базе H100 редко превышают 40 кВт, а традиционные CPU-стойки работают в диапазоне 7-15 кВт.

**Жидкостное охлаждение с фрикулингом (80% тепла уходит за пределы машинного зала) способно снизить PUE с 1.6 (воздушное) до 1.15-1.20 (достижимо с DLC + freecooling), уменьшить инфраструктурные потери с ~5 до ~1.82 МВт, тем самым обеспечить ежегодную экономию.**

---

## Плотность размещения: DGX B200 в стандартной 42U стойке

- **Размер DGX B200:** 10U
- **42U стойка:** 3U резервируется под PDU/мониторинг, остается 39U
- **ИТОГ:**
    - **3 сервера на 42U стойку**
    - **39 кВт на стойку** (3 × 13 кВт; реальное энергопотребление одного сервера — до 13 кВт по спецификации)

 Эту плотность физически можно увеличить, если использовать более плотные серверы, например, с жидкостным охлаждением.
 
## Плотность размещения 4U SXM B200 серверов в 42U стойке

- **Форм-фактор сервера:** 4U (176 мм высоты)
- **Мощность:** 13 кВт
- **42U стойка:** полная высота 42 RU
    
Максимальная плотность

**Резервирование:** 3 RU для PDU и управления  
**Доступное пространство:** 42 - 3 = 39 RU
**Максимум серверов:** 9 серверов
**Оставшееся место:** 3 RU (для доп. инструментария или оставляется свободным)
**Мощность на стойку:** 9 серверов×13 кВт=117 кВт/стойка

Это **ровно в 3 раза больше**, чем плотность 10U DGX B200 (39 кВт/стойка), и требует специализированного оборудования питания высокой мощности.

---

## Масштабирование, количество стоек и ИТ-мощность (3 года)

Для серверов Nvidia DGX B200:

|**Год**|**Серверы**|**42U Стойки**|**IT-мощность (МВт)**|
|---|---|---|---|
|0|100|34|1.30|
|1|300|100|3.90|
|2|500|167|6.50|
|3|700|234|9.10|

**Реальная конфигурация машинного зала на третий год:**
- **700 серверов**
- **234 полноценных стойки**
- **9.1 МВт чистой нагрузки на ИТ-оборудование**

Для серверов жидкостного охлаждения, размером 4U:

|**Год**|**Серверы**|**42U Стойки**|**IT-мощность (МВт)**|
|---|---|---|---|
|0|100|12|1.30|
|1|300|34|3.90|
|2|500|56|6.50|
|3|700|78|9.10|

---

## Требования к электропитанию объекта

### Воздушное охлаждение (100%)

- **PUE (Power Usage Effectiveness):** 1.6 (типовой предел для современных воздушных решений)
- **Общая мощность объекта:**  
    9.1 МВт×1.6=14.56 МВт
- **Инфраструктурные потери (охлаждение, UPS, трансформаторы):**  
    14.56−9.1=5.46 МВт

---

### Жидкостное охлаждение с фрикулингом (80-85% тепла уходит за пределы машинного зала)

- **PUE после внедрения жидкостного охлаждения:**  
    1.15-1.20 (достижимо с DLC + freecooling)
- **Общая мощность объекта:**  
    9.1 МВт×1.2=10.92 МВт
- **Инфраструктурные потери:**  
    10.92−9.1=1.82 МВт
    
---

## Рекомендации

- **Энергетическая инфраструктура**: закладывать мощности как минимум на 12–15 МВт; сроки подведения 12–24 месяца. Рассмотреть переход на 800 VDC архитектуру для rack-scale систем мощностью >200 кВт, как это планирует Nvidia.
- **Архитектура охлаждения**: необходим переход на Direct-to-Chip и фрикулинг (жидкостный контур + внешние теплообменники с воздушным/водяным охлаждением), резервное воздушное охлаждение на ~20%.
- **Гибридный подход**: сохранить воздушное охлаждение для некритичных компонентов (память, BMC, networking), что составляет 15-25% тепловой нагрузки
- **Расширяемость**: заложить в проект места под 2–3-кратный рост (до 1 500 серверов, 500+ стойки, 20–30 МВт по входной мощности).
- **Модульность**: проектировать инфраструктуру модульно с возможностью поэтапного ввода мощностей по мере роста AI-кластера.
- **Утилизация тепла**: рассмотреть интеграцию систем рекуперации тепла для отопления или промышленного использования, что дополнительно улучшает экономику проекта.

---

## Итоговый вывод

Мощность ЦОД или машинного зала на 700 серверов составит 9.1 МВт; в современных условиях воздушная система проигрывает по экономике и масштабируемости комбинированному жидкостному решению.

**Воздушное охлаждение (PUE ~1.6):** максимальный “нагрузочный” потолок, быстро устареет с ростом количества серверов и роста энергопотребления серверов.

**Жидкостное охлаждение + фрикулинг (PUE ~1.2):** стратегически наиболее правильный выбор при плотности от 500 серверов и выше с учёта преимуществ по масштабируемости, отказоустойчивости и компактности.


## Использованные материалы
1. [Datacenter Anatomy Part 2 – Cooling Systems](https://newsletter.semianalysis.com/p/datacenter-anatomy-part-2-cooling-systems?utm_source=publication-search)
2. [xAI’s Colossus 2 – First Gigawatt Datacenter In The World, Unique RL Methodology, Capital Raise – SemiAnalysis](https://semianalysis.com/2025/09/16/xais-colossus-2-first-gigawatt-datacenter/?access_token=eyJhbGciOiJFUzI1NiIsImtpZCI6InNlbWlhbmFseXNpcy5wYXNzcG9ydC5vbmxpbmUiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJzZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lIiwiYXpwIjoiS1NncVhBaGFmZmtwVjQzbmt0UU1INSIsImVudCI6eyJ1cmkiOlsiaHR0cHM6Ly9zZW1pYW5hbHlzaXMuY29tLzIwMjUvMDkvMTYveGFpcy1jb2xvc3N1cy0yLWZpcnN0LWdpZ2F3YXR0LWRhdGFjZW50ZXIvIl19LCJleHAiOjE3NjA2MzY5MjcsImlhdCI6MTc1ODA0NDkyNywiaXNzIjoiaHR0cHM6Ly9zZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lL29hdXRoIiwic2NvcGUiOiJmZWVkOnJlYWQgYXJ0aWNsZTpyZWFkIGFzc2V0OnJlYWQgY2F0ZWdvcnk6cmVhZCBlbnRpdGxlbWVudHMiLCJzdWIiOiJkZjA3NjYxZS04YTcyLTQwMDktODg0Yi04MjcwMzI2ZTVkYjkiLCJ1c2UiOiJhY2Nlc3MifQ.7bf7k4mkg8rf47lv6bs6G0HecIlaauY2S7Bc2UjLLBpIH8jb-lsoA99lQn8TlzsOBfVRsSUXrcY1-fGocDFAtw)
