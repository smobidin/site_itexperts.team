<!DOCTYPE html>
<html class="no-js" lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Архитектурный анализ межпроцессорной связи серверов - IT Experts Team</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="http://10.1.1.206:1313/1/">
  <meta property="og:site_name" content="IT Experts Team">
  <meta property="og:title" content="Архитектурный анализ межпроцессорной связи серверов">
  <meta property="og:description" content="Архитектурный анализ серверов на базе AMD 9005 Рассмотрим два дизайна серверов:
2 процессора AMD 9005 (CPU_0 и CPU_1), от каждого процессора разведено 5 слотов PCIe x16, в слоты от CPU_0 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, в слоты от CPU_1 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, таким образом у нас всего 8 ускорителей Nvidia H200 NVL и два адаптера Infiniband 400Gbps. 2 процессора AMD 9005 (CPU_0 и CPU_1), к каждому процессору подключен PCIe Switch Broadcom PEX89144 (BR_0 и BR_1 соответственно), подключение BR к CPU выполненно PCIe5 x16. От коммутатора BR_0 и BR_1 разведено 5 слотов PCIe x16, в слоты от BR_0 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, в слоты от BR_1 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, таким образом у нас всего 8 ускорителей Nvidia H200 NVL и два адаптера Infiniband 400Gbps. Краткий вывод Дизайн 1 (прямое подключение к CPU) является более оптимальным решением для задач инференса и fine-tuning больших моделей, требующих взаимодействия между двумя группами GPU. Прямое подключение через CPU обеспечивает более низкую латентность (15-25 мкс против 2-5 мкс через InfiniBand), лучшую интеграцию с технологиями GPUDirect и упрощённую топологию для tensor parallelism и pipeline parallelism.​">
  <meta property="og:locale" content="ru">
  <meta property="og:type" content="article">
    <meta property="article:published_time" content="2026-02-12T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-02-12T00:00:00+00:00">
    <meta property="article:tag" content="HW">
    <meta property="article:tag" content="PCIe">
    <meta property="article:tag" content="GPU">
    <meta property="article:tag" content="AI">

		
  <meta itemprop="name" content="Архитектурный анализ межпроцессорной связи серверов">
  <meta itemprop="description" content="Архитектурный анализ серверов на базе AMD 9005 Рассмотрим два дизайна серверов:
2 процессора AMD 9005 (CPU_0 и CPU_1), от каждого процессора разведено 5 слотов PCIe x16, в слоты от CPU_0 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, в слоты от CPU_1 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, таким образом у нас всего 8 ускорителей Nvidia H200 NVL и два адаптера Infiniband 400Gbps. 2 процессора AMD 9005 (CPU_0 и CPU_1), к каждому процессору подключен PCIe Switch Broadcom PEX89144 (BR_0 и BR_1 соответственно), подключение BR к CPU выполненно PCIe5 x16. От коммутатора BR_0 и BR_1 разведено 5 слотов PCIe x16, в слоты от BR_0 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, в слоты от BR_1 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, таким образом у нас всего 8 ускорителей Nvidia H200 NVL и два адаптера Infiniband 400Gbps. Краткий вывод Дизайн 1 (прямое подключение к CPU) является более оптимальным решением для задач инференса и fine-tuning больших моделей, требующих взаимодействия между двумя группами GPU. Прямое подключение через CPU обеспечивает более низкую латентность (15-25 мкс против 2-5 мкс через InfiniBand), лучшую интеграцию с технологиями GPUDirect и упрощённую топологию для tensor parallelism и pipeline parallelism.​">
  <meta itemprop="datePublished" content="2026-02-12T00:00:00+00:00">
  <meta itemprop="dateModified" content="2026-02-12T00:00:00+00:00">
  <meta itemprop="wordCount" content="2060">
  <meta itemprop="keywords" content="HW,PCIe,GPU,AI">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed">
		<a class="logo__link" href="/" title="ЭКСПЕРТЫ ИТ" rel="home">
			<div class="logo__item logo__imagebox">
					<img class="logo__img" src="/img/logo_team.png">
				</div><div class="logo__item logo__text">
					<div class="logo__title">ЭКСПЕРТЫ ИТ</div>
					<div class="logo__tagline">МЫ РОЖДАЕМ ИДЕИ, РАЗВИВАЕМ ПРОЕКТЫ И РЕШАЕМ ЗАДАЧИ</div>
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Архитектурный анализ межпроцессорной связи серверов</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 1 1 0 28 1 1 0 0 1 0-28m0 3a3 3 0 1 0 0 22 3 3 0 0 0 0-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class="meta__text" datetime="2026-02-12T00:00:00Z">2026-02-12</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0/" rel="category">Архитектура</a>, <a class="meta__link" href="/categories/%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5/" rel="category">Исследование</a>
	</span>
</div></div>
		</header>
		
<div class="post__toc toc">
	<div class="toc__title">Содержимое страницы</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#краткий-вывод">Краткий вывод</a></li>
    <li><a href="#детальное-сравнение-архитектур">Детальное сравнение архитектур</a></li>
    <li><a href="#дизайн-1-прямое-подключение-gpu-к-cpu">Дизайн 1: Прямое подключение GPU к CPU</a></li>
    <li><a href="#дизайн-2-pcie-коммутаторы-broadcom-pex89144">Дизайн 2: PCIe коммутаторы Broadcom PEX89144</a></li>
    <li><a href="#сильные-и-слабые-стороны">Сильные и слабые стороны</a></li>
    <li><a href="#дизайн-1-прямое-подключение-к-cpu">Дизайн 1: Прямое подключение к CPU</a></li>
    <li><a href="#дизайн-2-pcie-коммутаторы">Дизайн 2: PCIe коммутаторы</a></li>
    <li><a href="#использование-технологий-nvidia-gpudirect">Использование технологий NVIDIA GPUDirect</a></li>
    <li><a href="#gpudirect-rdma-с-infiniband">GPUDirect RDMA с InfiniBand</a></li>
    <li><a href="#gpudirect-p2p-peer-to-peer">GPUDirect P2P (Peer-to-Peer)</a></li>
    <li><a href="#оптимальность-для-задач-ai">Оптимальность для задач AI</a></li>
    <li><a href="#инференс-больших-моделей-с-model-parallelism">Инференс больших моделей с model parallelism</a></li>
    <li><a href="#fine-tuning-тренировка-больших-моделей">Fine-tuning (тренировка) больших моделей</a></li>
    <li><a href="#связь-между-группами-gpu-cpu-vs-infiniband">Связь между группами GPU: CPU vs InfiniBand</a></li>
    <li><a href="#через-cpu-infinity-fabric">Через CPU (Infinity Fabric)</a></li>
    <li><a href="#через-infiniband-400gbps">Через InfiniBand 400Gbps</a></li>
    <li><a href="#вердикт-cpu-infinity-fabric-более-оптимален">Вердикт: CPU (Infinity Fabric) более оптимален</a></li>
    <li><a href="#итоговые-рекомендации">Итоговые рекомендации</a></li>
    <li><a href="#дизайн-1-прямое-подключение-к-cpu--рекомендуется-для-задач-где-хватает-одного-сервера">Дизайн 1 (Прямое подключение к CPU) — <strong>Рекомендуется</strong> для задач, где хватает одного сервера</a></li>
    <li><a href="#дизайн-2-pcie-коммутаторы-broadcom--не-рекомендуется-для-ai-тренировкиинференса-рекомендуется-для-этих-же-задач-при-кластеризации-использовании-более-одного-сервера">Дизайн 2 (PCIe коммутаторы Broadcom) — <strong>Не рекомендуется</strong> для AI тренировки/инференса, <strong>Рекомендуется</strong> для этих же задач при кластеризации (использовании более одного сервера)</a></li>
    <li><a href="#использование-infiniband-адаптеров">Использование InfiniBand адаптеров</a></li>
  </ul>

  <ul>
    <li><a href="#краткий-вывод-1">Краткий вывод</a></li>
    <li><a href="#детальное-сравнение-архитектур-1">Детальное сравнение архитектур</a></li>
    <li><a href="#дизайн-1-прямое-подключение-gpu-к-cpu-1">Дизайн 1: Прямое подключение GPU к CPU</a></li>
    <li><a href="#дизайн-2-pcie-коммутаторы-broadcom">Дизайн 2: PCIe коммутаторы Broadcom</a></li>
    <li><a href="#рекомендация-для-intel-xeon-6700p">Рекомендация для Intel Xeon 6700P</a></li>
  </ul>
</nav>
	</div>
</div><div class="content post__content clearfix">
			<h1 id="архитектурный-анализ-серверов-на-базе-amd-9005">Архитектурный анализ серверов на базе AMD 9005</h1>
<p>Рассмотрим два дизайна серверов:</p>
<ol>
<li>2 процессора AMD 9005 (CPU_0 и CPU_1), от каждого процессора разведено 5 слотов PCIe x16, в слоты от CPU_0 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, в слоты от CPU_1 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, таким образом у нас всего 8 ускорителей Nvidia H200 NVL и два адаптера Infiniband 400Gbps.</li>
<li>2 процессора AMD 9005 (CPU_0 и CPU_1), к каждому процессору подключен PCIe Switch Broadcom PEX89144 (BR_0 и BR_1 соответственно), подключение BR к CPU выполненно PCIe5 x16. От коммутатора BR_0 и BR_1 разведено 5 слотов PCIe x16, в слоты от BR_0 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, в слоты от BR_1 установлена 4 ускорителя Nvidia H200 NVL, они объединены мостами NVLink 4-post, а так же адаптер Infiniband 400Gbps, таким образом у нас всего 8 ускорителей Nvidia H200 NVL и два адаптера Infiniband 400Gbps.</li>
</ol>
<h2 id="краткий-вывод">Краткий вывод</h2>
<p><strong>Дизайн 1 (прямое подключение к CPU)</strong> является более оптимальным решением для задач инференса и fine-tuning больших моделей, требующих взаимодействия между двумя группами GPU. Прямое подключение через CPU обеспечивает более низкую латентность (15-25 мкс против 2-5 мкс через InfiniBand), лучшую интеграцию с технологиями GPUDirect и упрощённую топологию для tensor parallelism и pipeline parallelism.​</p>
<h2 id="детальное-сравнение-архитектур">Детальное сравнение архитектур</h2>
<h2 id="дизайн-1-прямое-подключение-gpu-к-cpu">Дизайн 1: Прямое подключение GPU к CPU</h2>
<p><strong>Архитектура:</strong></p>
<ul>
<li>CPU_0 и CPU_1 AMD EPYC 9005, каждый с 5 слотами PCIe 5.0 x16</li>
<li>4x H200 NVL подключены к CPU_0, объединены NVLink 4-post</li>
<li>4x H200 NVL подключены к CPU_1, объединены NVLink 4-post</li>
<li>2x InfiniBand 400Gbps адаптера (по одному на каждый CPU)</li>
<li>Межсокетное соединение через Infinity Fabric (4 линка xGMI до 32 Gbps каждая)​</li>
</ul>
<p><strong>Пропускная способность:</strong></p>
<p>Процессоры AMD EPYC 9005 поддерживают до 128 PCIe Gen 5.0 линий в односокетной конфигурации и до 160 в двухсокетной. В двухсокетной системе часть линий используется для межсокетных Infinity Fabric подключений (обычно 64 линии или 4 линка x16), оставляя 64 линии PCIe на каждый процессор.​</p>
<p><strong>Внутри каждой группы из 4 GPU:</strong></p>
<ul>
<li>NVLink 4-way bridge: до 1.8 TB/s агрегированной пропускной способности на группу​</li>
<li>Каждый GPU H200 NVL имеет 900 GB/s NVLink bandwidth​</li>
<li>PCIe Gen5 x16: 128 GB/s на GPU для связи с CPU​</li>
</ul>
<p><strong>Между двумя группами GPU (через CPU):</strong></p>
<ul>
<li>PCIe Gen5 x16: 128 GB/s от GPU к CPU (64 GB/s в каждом направлении)​</li>
<li>Infinity Fabric между сокетами: приблизительно 37.9-42.6 GB/s агрегированной двунаправленной пропускной способности при использовании 4 линков​</li>
<li>Латентность межсокетного взаимодействия: 130-241 нс</li>
</ul>
<h2 id="дизайн-2-pcie-коммутаторы-broadcom-pex89144">Дизайн 2: PCIe коммутаторы Broadcom PEX89144</h2>
<p><strong>Архитектура:</strong></p>
<ul>
<li>CPU_0 и CPU_1 AMD EPYC 9005</li>
<li>PCIe Switch Broadcom PEX89144 (BR_0 и BR_1) подключены к CPU через PCIe5 x16</li>
<li>От каждого коммутатора разведено 5 слотов PCIe x16</li>
<li>4x H200 NVL подключены к BR_0, объединены NVLink 4-post</li>
<li>4x H200 NVL подключены к BR_1, объединены NVLink 4-post</li>
<li>2x InfiniBand 400Gbps адаптера</li>
</ul>
<p><strong>Пропускная способность:</strong></p>
<p>PEX89144 — это 144-линейный коммутатор PCIe Gen 5.0 с максимальной сырой пропускной способностью до 9,216 TB/s через все порты. Однако каждое upstream-подключение к CPU ограничено одной линком PCIe5 x16 (128 GB/s или 64 GB/s в каждом направлении).​</p>
<p><strong>Внутри каждой группы из 4 GPU:</strong></p>
<ul>
<li>NVLink 4-way bridge: до 1.8 TB/s (идентично Дизайну 1)​</li>
</ul>
<p><strong>Между двумя группами GPU (через PCIe Switch и CPU):</strong></p>
<ul>
<li>
<p>PCIe Gen5 x16: 128 GB/s от коммутатора BR_0 к CPU_0</p>
</li>
<li>
<p>Infinity Fabric между CPU: 37.9-42.6 GB/s</p>
</li>
<li>
<p>PCIe Gen5 x16: 128 GB/s от CPU_1 к коммутатору BR_1</p>
</li>
<li>
<p><strong>Дополнительная латентность коммутатора:</strong> 100-120 нс на каждый hop через PEX switch​</p>
</li>
<li>
<p><strong>Общая латентность:</strong> ~400-500 нс (две traversal через switches + межсокетная латентность + передача данных)​</p>
</li>
</ul>
<h2 id="сильные-и-слабые-стороны">Сильные и слабые стороны</h2>
<h2 id="дизайн-1-прямое-подключение-к-cpu">Дизайн 1: Прямое подключение к CPU</h2>
<p><strong>Сильные стороны:</strong></p>
<ol>
<li><strong>Минимальная латентность для GPU-to-GPU через сокеты:</strong> 15-25 мкс при использовании GPUDirect RDMA через PCIe​</li>
<li><strong>Упрощённая топология:</strong> меньше компонентов в data path означает меньше точек отказа и проще отладка</li>
<li><strong>Оптимальная поддержка GPUDirect RDMA:</strong> прямой путь от GPU к InfiniBand адаптеру без промежуточных коммутаторов обеспечивает лучшую производительность GPUDirect​</li>
<li><strong>Полоса пропускания PCIe доступна напрямую:</strong> каждый GPU имеет прямой доступ к полной пропускной способности PCIe x16 без конкуренции внутри коммутатора</li>
<li><strong>Лучшая производительность для tensor parallelism:</strong> прямое соединение через CPU с Infinity Fabric минимизирует латентность для синхронизации между GPU группами​</li>
</ol>
<p><strong>Слабые стороны:</strong></p>
<ol>
<li><strong>Ограничение по количеству линий PCIe:</strong> процессор должен распределить PCIe линии между GPU и другими устройствами, что может ограничить расширяемость</li>
<li><strong>Bottleneck Infinity Fabric:</strong> пропускная способность между сокетами (37.9-42.6 GB/s) значительно ниже, чем NVLink (1.8 TB/s)​</li>
<li><strong>NUMA эффекты:</strong> неравномерный доступ к памяти может повлиять на производительность​</li>
<li><strong>Сложность проектирования PCB:</strong> требует тщательной разводки PCIe линий на материнской плате</li>
</ol>
<h2 id="дизайн-2-pcie-коммутаторы">Дизайн 2: PCIe коммутаторы</h2>
<p><strong>Сильные стороны:</strong></p>
<ol>
<li><strong>Гибкость конфигурации:</strong> PCIe коммутатор позволяет более гибко распределять линии между устройствами​</li>
<li><strong>Масштабируемость:</strong> легче добавить дополнительные устройства без изменения CPU конфигурации</li>
<li><strong>Снижение нагрузки на PCIe линии CPU:</strong> CPU освобождается от необходимости напрямую управлять всеми GPU подключениями</li>
<li><strong>Потенциально лучшая балансировка нагрузки:</strong> коммутатор может динамически распределять bandwidth между портами​</li>
</ol>
<p><strong>Слабые стороны:</strong></p>
<ol>
<li><strong>Увеличенная латентность:</strong> добавление PCIe коммутатора добавляет 100-120 нс латентности на каждый hop​</li>
<li><strong>Bottleneck upstream подключения:</strong> все 4 GPU за одним коммутатором делят одно PCIe5 x16 подключение к CPU (128 GB/s), что создаёт узкое место при одновременной передаче данных​</li>
<li><strong>Усложнённая поддержка GPUDirect RDMA:</strong> дополнительный hop через PCIe switch может ухудшить производительность GPUDirect, особенно для GPU-to-GPU через сокеты​</li>
<li><strong>Увеличенное энергопотребление:</strong> дополнительные активные компоненты (PCIe switches) потребляют дополнительную энергию</li>
<li><strong>Потенциальные проблемы с PCIe топологией:</strong> пересечение межсокетной шины (Infinity Fabric/QPI) через PCIe switch может привести к серьёзным ограничениям производительности (250 MB/s - 1.1 GB/s в некоторых направлениях)​</li>
</ol>
<h2 id="использование-технологий-nvidia-gpudirect">Использование технологий NVIDIA GPUDirect</h2>
<h2 id="gpudirect-rdma-с-infiniband">GPUDirect RDMA с InfiniBand</h2>
<p><strong>GPUDirect RDMA</strong> позволяет InfiniBand адаптерам напрямую обращаться к памяти GPU, минуя CPU и системную память.​</p>
<p><strong>Дизайн 1 (преимущество):</strong></p>
<ul>
<li>Прямое подключение GPU к CPU и InfiniBand адаптера к тому же CPU обеспечивает оптимальный путь данных</li>
<li>Латентность GPUDirect RDMA: 1.7-2 мкс для небольших сообщений​</li>
<li>Пропускная способность: до 6-7.4 GB/s при использовании PCIe switch в той же секции, но может достигать близко к FDR peak (6.1 GB/s) при оптимальной топологии​</li>
</ul>
<p><strong>Дизайн 2 (недостаток):</strong></p>
<ul>
<li>Дополнительный PCIe switch в пути данных увеличивает латентность и может снизить эффективность GPUDirect​</li>
<li>Пересечение межсокетной шины через PCIe коммутатор приводит к драматическому падению производительности (250 MB/s для write, 1.1 GB/s для read)​</li>
</ul>
<h2 id="gpudirect-p2p-peer-to-peer">GPUDirect P2P (Peer-to-Peer)</h2>
<p><strong>Внутри каждой группы из 4 GPU:</strong></p>
<ul>
<li>Оба дизайна эквивалентны: NVLink 4-way bridge обеспечивает 900 GB/s bandwidth per GPU​</li>
<li>NVLink предпочтителен для P2P коммуникации внутри группы, так как обеспечивает в 7 раз большую пропускную способность по сравнению с PCIe Gen5​</li>
</ul>
<p><strong>Между группами GPU:</strong></p>
<ul>
<li><strong>Дизайн 1:</strong> GPUDirect P2P через CPU работает лучше благодаря прямому подключению и Infinity Fabric​</li>
<li><strong>Дизайн 2:</strong> дополнительные PCIe switches в пути значительно ухудшают производительность P2P между группами​</li>
</ul>
<h2 id="оптимальность-для-задач-ai">Оптимальность для задач AI</h2>
<h2 id="инференс-больших-моделей-с-model-parallelism">Инференс больших моделей с model parallelism</h2>
<p><strong>Требования:</strong></p>
<ul>
<li>Модели требующие более 564 GB памяти (4x141GB) должны быть разделены между двумя группами GPU</li>
<li>Tensor parallelism требует частой синхронизации между всеми GPU​</li>
<li>Критична низкая латентность для обмена активациями между слоями​</li>
</ul>
<p><strong>Рекомендация: Дизайн 1</strong></p>
<p>Tensor parallelism требует коммуникации после каждого слоя модели, что делает латентность критичным фактором. При TP=8 (все 8 GPU), требуется синхронизация между двумя группами GPU:​</p>
<ul>
<li><strong>Дизайн 1:</strong> латентность 15-25 мкс для GPU-to-GPU через CPU​</li>
<li><strong>Дизайн 2:</strong> латентность 400-500 нс только для traversal через switches, плюс время передачи данных​</li>
</ul>
<p>Для инференса, где каждый token требует forward pass через всю модель, накопленная латентность Дизайна 2 будет значительно хуже.​</p>
<h2 id="fine-tuning-тренировка-больших-моделей">Fine-tuning (тренировка) больших моделей</h2>
<p><strong>Требования:</strong></p>
<ul>
<li>Backward pass для градиентов требует интенсивной коммуникации</li>
<li>AllReduce операции для синхронизации градиентов​</li>
<li>Pipeline parallelism может снизить требования к bandwidth между группами​</li>
</ul>
<p><strong>Рекомендация: Дизайн 1 (с оговоркой)</strong></p>
<p>Для <strong>tensor parallelism в тренировке:</strong> Дизайн 1 значительно лучше из-за низкой латентности.​</p>
<p>Для <strong>pipeline parallelism:</strong> разница менее критична, так как PP передаёт активации только на границах stage, не требуя постоянной синхронизации. В этом случае оба дизайна работают приемлемо, но Дизайн 1 всё равно предпочтителен.​</p>
<p>Для <strong>3D parallelism</strong> (DP + PP + TP): Дизайн 1 обеспечивает лучшую общую производительность благодаря минимизации коммуникационных накладных расходов.​</p>
<h2 id="связь-между-группами-gpu-cpu-vs-infiniband">Связь между группами GPU: CPU vs InfiniBand</h2>
<h2 id="через-cpu-infinity-fabric">Через CPU (Infinity Fabric)</h2>
<p><strong>Пропускная способность:</strong> 37.9-42.6 GB/s bidirectional
<strong>Латентность:</strong> 130-241 нс для межсокетного доступа
<strong>Преимущества:</strong></p>
<ul>
<li>Низкая латентность для небольших сообщений</li>
<li>Прямой доступ к CPU памяти и кэшам</li>
<li>Поддержка когерентности памяти</li>
<li>Естественная интеграция с NUMA архитектурой​</li>
</ul>
<h2 id="через-infiniband-400gbps">Через InfiniBand 400Gbps</h2>
<p><strong>Пропускная способность:</strong> 400 Gbps = 50 GB/s per direction
<strong>Латентность GPUDirect RDMA:</strong> 1.7-5 мкс
<strong>Преимущества:</strong></p>
<ul>
<li>Выше bandwidth, чем Infinity Fabric (50 GB/s vs 37.9-42.6 GB/s)</li>
<li>Обход CPU для GPU-to-GPU коммуникации</li>
<li>Оптимизирован для RDMA операций​</li>
</ul>
<h2 id="вердикт-cpu-infinity-fabric-более-оптимален">Вердикт: CPU (Infinity Fabric) более оптимален</h2>
<p>Несмотря на то, что InfiniBand предлагает немного большую пропускную способность (50 GB/s vs 42.6 GB/s), связь через CPU является более оптимальной по следующим причинам:</p>
<ol>
<li><strong>Значительно более низкая латентность:</strong> 130-241 нс через Infinity Fabric против 1.7-5 мкс через InfiniBand RDMA​</li>
<li><strong>Для tensor parallelism латентность критичнее bandwidth:</strong> синхронизация после каждого слоя требует низкой латентности, не высокой пропускной способности​</li>
<li><strong>NCCL автоматически выбирает оптимальный путь:</strong> библиотека NCCL (используемая PyTorch и другими фреймворками) автоматически выберет PCIe/Infinity Fabric для intra-node коммуникации и InfiniBand для inter-node​</li>
<li><strong>InfiniBand лучше использовать для inter-node:</strong> InfiniBand оптимален для связи между физически разными серверами, не между группами GPU внутри одного сервера​</li>
</ol>
<h2 id="итоговые-рекомендации">Итоговые рекомендации</h2>
<h2 id="дизайн-1-прямое-подключение-к-cpu--рекомендуется-для-задач-где-хватает-одного-сервера">Дизайн 1 (Прямое подключение к CPU) — <strong>Рекомендуется</strong> для задач, где хватает одного сервера</h2>
<p><strong>Оптимален для:</strong></p>
<ul>
<li>Инференс больших моделей с tensor parallelism (TP=8)</li>
<li>Fine-tuning с высокими требованиями к латентности</li>
<li>Приложения, требующие минимальной латентности GPU-to-GPU</li>
<li>Использование GPUDirect RDMA с максимальной эффективностью</li>
</ul>
<p><strong>Ключевое преимущество:</strong> минимальная латентность для синхронизации между группами GPU при использовании tensor parallelism.</p>
<h2 id="дизайн-2-pcie-коммутаторы-broadcom--не-рекомендуется-для-ai-тренировкиинференса-рекомендуется-для-этих-же-задач-при-кластеризации-использовании-более-одного-сервера">Дизайн 2 (PCIe коммутаторы Broadcom) — <strong>Не рекомендуется</strong> для AI тренировки/инференса, <strong>Рекомендуется</strong> для этих же задач при кластеризации (использовании более одного сервера)</h2>
<p><strong>Может быть приемлем для:</strong></p>
<ul>
<li>Pipeline parallelism с большими micro-batch размерами</li>
<li>Сценарии, где GPU группы работают независимо</li>
<li>Применения, требующие гибкой конфигурации PCIe устройств</li>
<li>При необходимости использовании более, чем одного сервера (кластеризации)</li>
</ul>
<p><strong>Критические недостатки:</strong> увеличенная латентность и bottleneck upstream подключения делают этот дизайн субоптимальным для большинства AI workloads.</p>
<h2 id="использование-infiniband-адаптеров">Использование InfiniBand адаптеров</h2>
<p>InfiniBand 400Gbps адаптеры следует использовать <strong>для inter-node коммуникации</strong> (между физически разными серверами), а не для связи между группами GPU внутри одного сервера. Внутри сервера оптимальна связь через CPU с использованием Infinity Fabric.​</p>
<h1 id="архитектурный-анализ-серверов-на-базе-intel-xeon-6700p">Архитектурный анализ серверов на базе Intel Xeon 6700P</h1>
<p>В этом сценарии рассмотрим процессоры &ldquo;Mainstream&rdquo; сегмента (сокет LGA 4710), такие как <strong>Xeon 6780P</strong> или <strong>6756P</strong>.</p>
<h2 id="краткий-вывод-1">Краткий вывод</h2>
<p><strong>Дизайн 1 (Прямое подключение к CPU)</strong> является единственным правильным выбором для этой платформы, но с оговоркой: у нас остается критически мало свободных линий PCIe.</p>
<p><strong>Дизайн 2 (через коммутатор)</strong> здесь <strong>категорически неэффективен</strong>. Из-за того, что процессоры Xeon 6700P имеют мощную межсокетную связь (UPI), но подключаются к коммутатору всего одной линией PCIe x16, мы искусственно &ldquo;душим&rdquo; производительность всей группы из 4-х GPU при попытке передать данные на соседний процессор.</p>
<h2 id="детальное-сравнение-архитектур-1">Детальное сравнение архитектур</h2>
<h2 id="дизайн-1-прямое-подключение-gpu-к-cpu-1">Дизайн 1: Прямое подключение GPU к CPU</h2>
<p><strong>Архитектура:</strong></p>
<ul>
<li><strong>CPU:</strong> 2x Intel Xeon 6700P (Granite Rapids-SP).</li>
<li><strong>PCIe линии:</strong> В двухсокетной конфигурации каждый процессор предоставляет <strong>88 линий PCIe 5.0</strong>.<a href="https://www.intel.com/content/dam/www/central-libraries/us/en/documents/2025-02/xeon-6-granite-rapids-product-brief.pdf"></a>​
<ul>
<li><em>Важно:</em> Это меньше, чем у 6900P (96 линий) и AMD EPYC 9005 (128+ линий).</li>
</ul>
</li>
<li><strong>Потребление:</strong>
<ul>
<li>4 x GPU (x16) = 64 линии.</li>
<li>1 x IB 400G (x16) = 16 линий.</li>
<li><strong>Итого занято:</strong> 80 линий из 88 доступных.</li>
</ul>
</li>
<li><strong>Остаток:</strong> Всего <strong>8 линий</strong> на процессор. Этого хватит только для пары загрузочных NVMe дисков (x4) и базового управления. Подключить дополнительные NVMe массивы для кэширования данных уже не получится без дополнительных коммутаторов.</li>
</ul>
<p><strong>Межсокетная связь (UPI):</strong></p>
<ul>
<li>Процессоры Xeon 6700P имеют <strong>4 линка UPI 2.0</strong> (против 6 у серии 6900P).<a href="https://www.nextplatform.com/2025/02/24/intel-rounds-out-granite-rapids-xeon-6-with-a-slew-of-chips/"></a>​</li>
<li>Скорость: 24 GT/s на линк.</li>
<li>Агрегированная пропускная способность: <strong>~96-100 ГБ/с</strong> (полезной нагрузки в одну сторону).</li>
<li>Это все еще <strong>в 2 раза быстрее</strong>, чем InfiniBand 400G (50 ГБ/с).</li>
</ul>
<p><strong>Сильные стороны:</strong></p>
<ol>
<li><strong>Максимальная утилизация межсокетной шины:</strong> 4 GPU могут одновременно отправлять данные на другой сокет с суммарной скоростью до ~100 ГБ/с (ограничено UPI).</li>
<li><strong>Низкая латентность:</strong> Прямой путь данных.</li>
</ol>
<p><strong>Слабые стороны:</strong></p>
<ol>
<li><strong>Дефицит расширения:</strong> Система получается &ldquo;закрытой&rdquo;. Свободных линий для добавления storage-адаптеров почти нет.</li>
</ol>
<h2 id="дизайн-2-pcie-коммутаторы-broadcom">Дизайн 2: PCIe коммутаторы Broadcom</h2>
<p><strong>Архитектура:</strong></p>
<ul>
<li>4 GPU и 1 сетевая карта висят на коммутаторе PEX89144.</li>
<li>Коммутатор подключен к CPU одним линком PCIe 5.0 x16.</li>
</ul>
<p><strong>Критический недостаток (узкое место Uplink):</strong><br>
В этой схеме все 4 GPU &ldquo;борются&rdquo; за <strong>одну</strong> линию PCIe x16 (64 ГБ/с), чтобы передать данные на соседний CPU.</p>
<ul>
<li>В Дизайне 1 пропускная способность между группами GPU составляла <strong>~100 ГБ/с</strong> (лимит UPI).</li>
<li>В Дизайне 2 пропускная способность падает до <strong>64 ГБ/с</strong> (лимит Uplink x16), так как все данные должны пройти через узкое горлышко между коммутатором и CPU.</li>
</ul>
<p>Это делает Дизайн 2 бессмысленным: мы используем дорогие GPU и мощные CPU, но соединяете их &ldquo;тонкой трубочкой&rdquo;.</p>
<h2 id="рекомендация-для-intel-xeon-6700p">Рекомендация для Intel Xeon 6700P</h2>
<p><strong>Выбирайте Дизайн 1 (Прямое подключение).</strong></p>
<p>Даже с учетом меньшего количества линий UPI (4 шт.) по сравнению с топовой серией, связь через CPU все равно значительно быстрее и эффективнее, чем использование коммутатора с одним аплинком.</p>
<p><strong>Связь двух групп ускорителей:</strong></p>
<ul>
<li><strong>Через CPU (UPI):</strong> Оптимальна. Обеспечивает ~100 ГБ/с пропускной способности.</li>
<li><strong>Через InfiniBand:</strong> Субоптимальна (50 ГБ/с). Используйте IB только для связи между серверами.</li>
</ul>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 0 0 0-6 3 3 0 0 0 0 6"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/hw/" rel="tag">HW</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/pcie/" rel="tag">PCIe</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/gpu/" rel="tag">GPU</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/ai/" rel="tag">AI</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<p class="authorbox__warning">
		<strong>WARNING:</strong> Authorbox is activated, but [Author] parameters are not specified.
	</p>
</div>



			</div>
			<aside class="sidebar sidebar--left"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<input class="widget-search__field" type="search" placeholder="Поиск…" value="" name="q" aria-label="Поиск…">
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="http://10.1.1.206:1313/">
	</form>
</div>
<div class="widget-categories widget">
	<h4 class="widget__title">Категории</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item">
				<a class="widget__link" href="/categories/%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0/">Архитектура</a>&nbsp;
				<span class="widget__counter widget__counter--bubble">1</span>
				</li>
			<li class="widget__item">
				<a class="widget__link" href="/categories/%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5/">Исследование</a>&nbsp;
				<span class="widget__counter widget__counter--bubble">1</span>
				</li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Теги</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/tags/ai/" title="AI">AI (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/gpu/" title="GPU">GPU (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/hw/" title="HW">HW (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/pcie/" title="PCIe">PCIe (1)</a>
	</div>
</div>
<div class="widget-social widget">
	<h4 class="widget-social__title widget__title">Социальные сети</h4>
	<div class="widget-social__content widget__content">
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="Telegram" rel="noopener noreferrer" href="https://t.me/@sobidin" target="_blank">
				<svg class="widget-social__link-icon icon icon-telegram" width="24" height="24" viewBox="0 0 132 110"><path fill="#ddd" d="M50 103c-4 0-3-1-5-5L34 60l88-52"/><path fill="#aaa" d="M50 103c3 0 4-1 6-3l16-16-20-12"/><path fill="#fff" d="M52 72l48 36c6 3 10 2 11-5l20-93c2-8-3-11-8-9L7 45c-8 4-8 8-1 10l29 9 69-43c3-2 6-1 4 1"/></svg>
				<span>Telegram</span>
			</a>
		</div>
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="GitHub" rel="noopener noreferrer" href="https://github.com/smobidin" target="_blank">
				<svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0c-106.1 0-192 85.8-192 191.7 0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2 0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8 0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7 0 0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4 0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5 0 25.6-.2 46.3-.2 52.6 0 5.1 3.5 11.1 13.2 9.2 76.2-25.5 131.2-97.3 131.2-182 0-105.9-86-191.7-192-191.7z"/></svg>
				<span>GitHub</span>
			</a>
		</div>

		
	</div>
</div>

</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2026 IT Experts Team.
			<span class="footer__copyright-credits">Сгенерировано с использованием <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> и темы <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a>.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>